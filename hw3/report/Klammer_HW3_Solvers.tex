\documentclass[12pt, a4paper]{article}
\title{Homework 3: Solvers}
\author{Christopher Klammer \\ \small\texttt{cklammer@andrew.cmu.edu}}

% A pretty common set of packages
\usepackage[margin=2.5cm]{geometry}
\usepackage[T1]{fontenc}
\usepackage{graphicx}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{bm}
\usepackage{color}
\usepackage{float}
\usepackage{bm}
\usepackage{physics}
\usepackage{subcaption}

\begin{document}
\maketitle
\section{Linear SLAM}
\subsection{Measurement Function}
\subsubsection{Odometry Measurement}
\textbf{The odometry measurement should just be the vector from the pose at time $t$ to the pose at time $t+1$}
$$h_{o}(\vectorbold{r^t}, \vectorbold{r^{t+1}}) = 
\begin{bmatrix} r^{t+1}_x - r^t_x  & r^{t+1}_y - r^t_y \end{bmatrix}^T$$ 

$$H_{o} = \begin{bmatrix}
    \frac{\partial r^{t+1}_x - r^t_x}{\partial r^t_x} & \frac{r^{t+1}_x - r^t_x}{\partial r^t_y}& \frac{\partial r^{t+1}_x - r^t_x}{\partial r^{t+1}_{x}} & \frac{\partial r^{t+1}_x - r^t_x}{\partial r_y^{t+1}} \\
    \frac{\partial r^{t+1}_y - r^t_y}{\partial r^t_x} & \frac{\partial r^{t+1}_y - r^t_y}{\partial r^t_y}& \frac{\partial r^{t+1}_y - r^t_y}{\partial r^{t+1}_{x}} & \frac{\partial r^{t+1}_y - r^t_y}{\partial r_y^{t+1}}
\end{bmatrix}$$

$$H_{o} = \begin{bmatrix}
    -1 & 0 & 1 & 0 \\
    0 & -1 & 0 & 1
\end{bmatrix}$$

\subsubsection{Landmark Measurement}
\textbf{This will be extremely similar, just find the vector from the robot's position to the landmark}
$$h_{l}(\vectorbold{r^t}, \vectorbold{l^k}) = 
\begin{bmatrix} l^k_x - r^t_x  & l^k_y - r^t_y \end{bmatrix}^T$$


$$H_l = \begin{bmatrix}
    \frac{\partial l^k_x - r^t_x}{\partial r^t_x} & \frac{\partial l^k_x - r^t_x}{\partial r^t_y} & \frac{\partial l^k_x - r^t_x}{\partial l^k_x} & \frac{\partial l^k_x - r^t_x}{\partial l^k_y} \\
    \frac{\partial l^k_y - r^t_y }{\partial r^t_x} & \frac{\partial l^k_y - r^t_y }{\partial r^t_y} & \frac{\partial l^k_y - r^t_y }{\partial l^k_x} & \frac{\partial l^k_y - r^t_y }{\partial l^k_y}
\end{bmatrix}$$

$$H_l = \begin{bmatrix}
    -1 & 0 & 1 & 0 \\
    0  & -1 & 0 & -1
\end{bmatrix}$$

\subsection{Solvers and Sparsity}
\subsubsection{2D Linear Data}
\textbf{Timing:}

\begin{tabular}{|c | c | c | c | c|}
    \hline
    \textbf{PInv} & \textbf{LU} & \textbf{LU COLAMD} & \textbf{QR} & \textbf{QR COLAMD} \\
    \hline
    \hline
    4.900 sec & 0.073 sec & 0.186 sec & 1.074 sec & 1.113 sec \\
    \hline
\end{tabular}

\begin{figure}
    \center
    \includegraphics[width=0.75\textwidth]{linear_results/PInv_Traj.png}
    \caption{Trajectory for PInv Solver}
\end{figure}

\begin{figure}
    \center
    \includegraphics[width=0.75\textwidth]{linear_results/LU_Factor_Traj.png}
    \caption{Trajectory for LU Solver}
\end{figure}

\begin{figure}
    \center
    \includegraphics[width=0.75\textwidth]{linear_results/LU_Factor_Sparsity.png}
    \caption{Sparsity for LU Solver}
\end{figure}

\begin{figure}
    \center
    \includegraphics[width=0.75\textwidth]{linear_results/LU_COLAMD_Traj.png}
    \caption{Trajectory for LU COLAMD Solver}
\end{figure}

\begin{figure}
    \center
    \includegraphics[width=0.75\textwidth]{linear_results/LU_COLAMD_Sparsity.png}
    \caption{Sparsity for LU COLAMD Solver}
\end{figure}

\begin{figure}
    \center
    \includegraphics[width=0.75\textwidth]{linear_results/QR_Traj.png}
    \caption{Trajectory for QR Solver}
\end{figure}

\begin{figure}
    \center
    \includegraphics[width=0.75\textwidth]{linear_results/QR_Sparsity.png}
    \caption{Sparsity for QR Solver}
\end{figure}

\begin{figure}
    \center
    \includegraphics[width=0.75\textwidth]{linear_results/QR_COLAMD_Traj.png}
    \caption{Trajectory for QR COLMAD Solver}
\end{figure}

\begin{figure}
    \center
    \includegraphics[width=0.75\textwidth]{linear_results/QR_COLAMD_Sparsity.png}
    \caption{Sparsity for QR COLMAD Solver}
\end{figure}

\clearpage
\subsubsection{2D Linear Discussion}
First off, we see here that the psuedo-inverse is the slowest, the LU solver is the fastest, and the QR solver is in the middle. We also see that the COLAMD methods do not cause any speedup.

In the case of a linear trajectory, we do not have any redundant observations. Each landmark that we see is relatively new and, since the trajectory is fairly linear, we see that our factorized matricies are sparse and close to diagonal. Since this is the case, we do not see performance increase by using the COLAMD ordering. In my opinion, it seems that no further sparsification can be gained with COLAMD so the reordering acts as an overhead making the performance worse.

\clearpage
\subsubsection{2D Linear Loop Data}
\textbf{Timing:}

\begin{tabular}{|c | c | c | c | c|}
    \hline
    \textbf{PInv} & \textbf{LU} & \textbf{LU COLAMD} & \textbf{QR} & \textbf{QR COLAMD} \\
    \hline
    \hline
    0.451 sec & 0.056 sec & 0.009 sec & 0.616 sec & 0.063 sec \\
    \hline
\end{tabular}

\begin{figure}
    \center
    \includegraphics[width=0.75\textwidth]{linear_loop_results/loop_pinv.png}
    \caption{Trajectory for PInv Solver}
\end{figure}

\begin{figure}
    \center
    \includegraphics[width=0.75\textwidth]{linear_loop_results/loop_lu.png}
    \caption{Trajectory for LU Solver}
\end{figure}

\begin{figure}
    \center
    \includegraphics[width=0.75\textwidth]{linear_loop_results/lu_sparsity_loop.png}
    \caption{Sparsity for LU Solver}
\end{figure}

\begin{figure}
    \center
    \includegraphics[width=0.75\textwidth]{linear_loop_results/lu_colamd.png}
    \caption{Trajectory for LU COLAMD Solver}
\end{figure}

\begin{figure}
    \center
    \includegraphics[width=0.75\textwidth]{linear_loop_results/lu_colamd_sparsity.png}
    \caption{Sparsity for LU COLAMD Solver}
\end{figure}

\begin{figure}
    \center
    \includegraphics[width=0.75\textwidth]{linear_loop_results/loop_qr.png}
    \caption{Trajectory for QR Solver}
\end{figure}

\begin{figure}
    \center
    \includegraphics[width=0.75\textwidth]{linear_loop_results/qr_sparsity_loop.png}
    \caption{Sparsity for QR Solver}
\end{figure}

\begin{figure}
    \center
    \includegraphics[width=0.75\textwidth]{linear_loop_results/qr_colamd.png}
    \caption{Trajectory for QR COLMAD Solver}
\end{figure}

\begin{figure}
    \center
    \includegraphics[width=0.75\textwidth]{linear_loop_results/qr_colamd_sparsity.png}
    \caption{Sparsity for QR COLMAD Solver}
\end{figure}

\clearpage
\subsubsection{2D Linear Loop Discussion}
In general, we see the same relative ordering in that the psuedo-inverse is the slowest, the LU solver is the quickest, and the QR solver is in the middle. However, in this case, we see that COLAMD offers massive speedups.


In this case, we appear to move in a loop where we visit the same landmarks multiple times and we also see similar odometry measurements as well because we are moving in the same loop. This creates a much more dense matrix, especially compared to the linear case.

Here, we see large speedups in performance when using the COLAMD ordering which helps sparsify the matrix by removing more redudant measurements. For example, seeing the same landmark from the same pose is probably not useful, we can essentially remove or generally combine these redundant rows in order to create a more sparse representation. 

\section{Non-Linear SLAM}
\subsection{Measurement Function}
\subsubsection{Landmark Measurement}
\textbf{Notation:}
$$dx = l_x^k - r_x^t$$
$$dy = l_y^k - r_y^t$$
\\
\textbf{Calculation:}
$$H_{l} = \begin{bmatrix}
    \frac{\partial \theta}{\partial r^t_x} & \frac{\partial \theta}{\partial r^t_y}& \frac{\partial \theta}{\partial l_x^k} & \frac{\partial \theta}{\partial l_y^k} \\
    \frac{\partial d}{\partial r^t_x} & \frac{\partial d}{\partial r^t_y}& \frac{\partial d}{\partial l_x^k} & \frac{\partial d}{\partial l_y^k}
\end{bmatrix}$$
\\
$$\frac{\partial \theta}{\partial r_x^t} = \frac{dx^2}{dx^2 + dy^2} * \frac{dy}{dx^2} = \frac{dy}{dx^2 + dy^2}$$

$$\frac{\partial \theta}{\partial r_y^t} = \frac{dx^2}{dx^2 + dy^2} * -\frac{dx}{dx^2} = -\frac{dx}{dx^2 + dy^2}$$

$$\frac{\partial \theta}{\partial l_x^k} = \frac{dx^2}{dx^2 + dy^2} * \frac{-dy}{dx^2} = -\frac{dy}{dx^2 + dy^2}$$

$$\frac{\partial \theta}{\partial l_y^k} = \frac{dx^2}{dx^2 + dy^2} * \frac{dx}{dx^2} = \frac{dx}{dx^2 + dy^2}$$

$$\frac{\partial d}{\partial r_x^t} = \frac{1}{2} * (dx^2 + dy^2)^{-\frac{1}{2}} * -2 * dx = \frac{-dx}{(dx^2 + dy^2)^{1/2}}$$

$$\frac{\partial d}{\partial r_y^t} = \frac{1}{2} * (dx^2 + dy^2)^{-\frac{1}{2}} * -2 * dy = \frac{-dy}{(dx^2 + dy^2)^{1/2}}$$

$$\frac{\partial d}{\partial l_x^k} = \frac{1}{2} * (dx^2 + dy^2)^{-\frac{1}{2}} * 2 * dx = \frac{dx}{(dx^2 + dy^2)^{1/2}}$$

$$\frac{\partial d}{\partial l_y^k} = \frac{1}{2} * (dx^2 + dy^2)^{-\frac{1}{2}} * 2 dy = \frac{dy}{(dx^2 + dy^2)^{1/2}}$$


$$H_{l} = \begin{bmatrix}
    \frac{dy}{dx^2 + dy^2} & \frac{-dx}{dx^2 + dy^2} & \frac{-dy}{dx^2 + dy^2} & \frac{dx}{dx^2 + dy^2} \\
    \frac{-dx}{(dx^2 + dy^2)^{1/2}} & \frac{-dy}{(dx^2 + dy^2)^{1/2}}& \frac{dx}{(dx^2 + dy^2)^{1/2}} & \frac{dy}{(dx^2 + dy^2)^{1/2}}
\end{bmatrix}$$

\clearpage

\subsection{Solver}

\subsubsection{Linear vs. Non-Linear SLAM}
The difference between the linear and non-linear SLAM problem lies in a few key aspects here. I will first discuss linear SLAM and why it is simpler that non-linear SLAM. Then I will discuss the complexities of non-linear SLAM and why the optimization must be formulated differently. 

Linear least squares is a fairly simple formulation, where $h_i(x) = Hx + h_0$ and we assume that there is no rotations that would make the formulation non-linear. At this point we can solve for x that minimizes the least squares solution $||Ax - b||^2$ where $A$ is the jacobian of our measurement function, $x$ is the state and b is $z - h_0$ or the true measurement with some constant from the measurement function.

The first being that with non-linear SLAM, we are inherently working with a non-convex problem with a non-direct solution. Thus, we must perform an approximation for the measurement function. This approximation is carried out in the form of a first order taylor series expansion about a particular linearization point $x^0$ of the form:

$$z_{est} = h_i(x) = h(x^0 + \delta) = h(x^0) + \frac{\partial h(x)}{x} * (x - x^0)$$ 

Here, this linearization point needs to be dictated by essentially a current guess or estimate and will be iteratively improved to move the estimated and actual measurements closer together. This methodology is inherently dependent on a good initialization and its covergence (or at least its speed of convergence) is highly sensitive to the initial estimate of the state $x^0$. Knowing this formulation we can find the residual of our measurement and estimated approximation in order to iteratively solve: 

$$||z_{est} = z||^2 = ||h(x^0) + \frac{\partial h(x)}{\partial x}|_{x=x_0} * (x - x^0) - z||^2 = ||A\delta - b||^2$$

where:

$$A = h(x^0) + \frac{\partial h(x)}{\partial x}|_{x=x_0}$$
$$b = z$$ 
$$\delta = x-x_0$$

\clearpage
\subsubsection{Visualization}
\begin{figure}[!htb]
    \center
    \includegraphics[width=0.75\textwidth]{nonlinear_results/QRCOLAMD_BeforeOptim.png}
    \caption{QR COLAMD Before Optimization}
\end{figure}

\begin{figure}[!htb]
    \center
    \includegraphics[width=0.75\textwidth]{nonlinear_results/QRCOLAMD_AfterOptim.png}
    \caption{QR COLAMD After Optimization}
\end{figure}

\end{document}